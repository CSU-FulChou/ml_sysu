{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 作业02：Fashion Mnist实现图像分类\n",
    "\n",
    "## 1 概述\n",
    "Fashion-MNIST是一个替代MNIST手写数字集的图像数据集，它是由Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自10种类别的共7万个不同商品的正面图片。Fashion-MNIST的大小、格式和训练集/测试集划分与原始的MNIST完全一致。数据集按照60000/10000的比例进行训练测试数据划分，全部数据均为28x28的灰度图片。\n",
    "\n",
    "**基本要求**：\n",
    "1. 使用卷积神经网络完成Fashion-MNIST的分类任务。\n",
    "2. 在README文件中描述所使用的模型的结构、优化器、损失函数和超参数等信息，以及模型在训练集和测试集上的最优结果。\n",
    "3. 对模型中训练过程损失函数的变化趋势可视化\n",
    "\n",
    "**加分项**：可以对模型进行优化（包括增加层数，使用残差连接，调整超参数等），或者对模型的训练过程/结果进行可视化（例如模型损失函数在训练过程中的变化趋势，或者参数的分布随训练批次的变化趋势等）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 实现模型\n",
    "\n",
    "### 2.1 数据集加载\n",
    "可以通过飞桨自带的`paddle.vision.dataset`进行数据集加载，并且通过`paddle.vision.transforms`对数据进行预处理，例如对数据进行归一化。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install optuna"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import paddle\n",
    "print(paddle.__version__)\n",
    "import time \n",
    "import paddle.vision.transforms as T\n",
    "\n",
    "transform = T.Compose([T.Normalize(mean=[127.5],\n",
    "                                   std=[127.5],\n",
    "                                   data_format='CHW')])\n",
    "\n",
    "# 对数据进行归一化\n",
    "train_dataset = paddle.vision.datasets.FashionMNIST(mode='train', transform=transform)\n",
    "test_dataset = paddle.vision.datasets.FashionMNIST(mode='test', transform=transform)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 模型搭建\n",
    "\n",
    "使用paddle的接口搭建神经网络模型。以LeNet模型为例，该模型与1998年提出，包含了深度学习图像处理相关的基本模块。\n",
    "\n",
    "LeNet模型一共有7层，包括2个**卷积层**，2个**池化层**和3个**全连接层**。通过连续使用卷积层和池化层提取图像特征。Paddle中提供了相应的接口，可以快速搭建网络模型：\n",
    "1. 卷积层：`paddle.nn.Conv2D`；\n",
    "2. 池化层：`paddle.nn.MaxPool2D`；\n",
    "3. 全连接层：`paddle.nn.Linear`。\n",
    "\n",
    "### **作业要求**：搭建神经网络，完成`__init__`和`forward`函数，并在README文件中详细描述所使用模型的信息。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import paddle.nn.functional as F\n",
    "import paddle.nn as NN\n",
    "class MyNet(paddle.nn.Layer):    \n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layer1 = NN.Sequential(   \n",
    "            NN.Conv2D(1, 16, kernel_size=5, padding=2),\n",
    "            NN.BatchNorm2D(16), \n",
    "            NN.ReLU()) #16, 28, 28\n",
    "        self.pool1=NN.MaxPool2D(2) #16, 14, 14\n",
    "        self.layer2 = NN.Sequential(\n",
    "            NN.Conv2D(16, 32, kernel_size=3),\n",
    "            NN.BatchNorm2D(32),\n",
    "            NN.ReLU())#32, 12, 12\n",
    "        self.layer3 = NN.Sequential(\n",
    "            NN.Conv2D(32, 64, kernel_size=3),\n",
    "            NN.BatchNorm2D(64),\n",
    "            NN.ReLU()) #64, 10, 10\n",
    "        self.pool2=NN.MaxPool2D(2)  #64, 5, 5\n",
    "        self.fc = NN.Linear(5*5*64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #print(out.shape)\n",
    "        out=self.pool1(out)\n",
    "        #print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        #print(out.shape)\n",
    "        out=self.layer3(out)\n",
    "        #print(out.shape)\n",
    "        out=self.pool2(out)\n",
    "        # print(out.shape)\n",
    "        out = paddle.reshape(out,[-1,5*5*64])\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import paddle.nn.functional as F # 组网相关的函数，如conv2d, relu...\n",
    "import numpy as np\n",
    "from paddle.nn.layer.common import Dropout \n",
    "from paddle.vision.transforms import Compose, Resize, Transpose, Normalize, ToTensor\n",
    "from paddle.vision.datasets import Cifar10\n",
    "\n",
    "# 构建ResNet网络\n",
    "# Sequential：顺序容器，子Layer将按构造函数参数的顺序添加到此容器中，传递给构造函数的参数可以Layers或可迭代的name Layer元组\n",
    "from paddle.nn import Sequential, Conv2D, ReLU, MaxPool2D, Linear, Dropout, Flatten, BatchNorm2D, AvgPool2D\n",
    "\n",
    "#构建模型\n",
    "class Residual(paddle.nn.Layer):\n",
    "    def __init__(self, in_channel, out_channel, use_conv1x1=False, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(in_channel, out_channel, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = Conv2D(out_channel, out_channel, kernel_size=3, padding=1)\n",
    "        if use_conv1x1: #使用1x1卷积核\n",
    "            self.conv3 = Conv2D(in_channel, out_channel, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.batchNorm1 = BatchNorm2D(out_channel)\n",
    "        self.batchNorm2 = BatchNorm2D(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.batchNorm1(self.conv1(x)))\n",
    "        y = self.batchNorm2(self.conv2(y))\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "        out = F.relu(y+x) #核心代码\n",
    "        return out\n",
    "def ResNetBlock(in_channel, out_channel, num_layers, is_first=False):\n",
    "    if is_first:\n",
    "        assert in_channel == out_channel\n",
    "    block_list = []\n",
    "    for i in range(num_layers):\n",
    "        if i == 0 and not is_first:\n",
    "            block_list.append(Residual(in_channel, out_channel, use_conv1x1=True, stride=2))\n",
    "        else:\n",
    "            block_list.append(Residual(out_channel, out_channel))\n",
    "    resNetBlock = Sequential(*block_list) #用*号可以把list列表展开为元素\n",
    "    return resNetBlock\n",
    "\n",
    "class ResNet50(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.b1 = Sequential(\n",
    "                    Conv2D(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                    BatchNorm2D(64), \n",
    "                    ReLU(),\n",
    "                    MaxPool2D(kernel_size=3, stride=2, padding=1))\n",
    "        self.b2 = ResNetBlock(64, 64, 3, is_first=True)\n",
    "        self.b3 = ResNetBlock(64, 128, 4)\n",
    "        self.b4 = ResNetBlock(128, 256, 6)\n",
    "        self.b5 = ResNetBlock(256, 512, 3)\n",
    "        self.AvgPool = AvgPool2D(2)\n",
    "        self.flatten = Flatten()\n",
    "        self.Linear = Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.AvgPool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.Linear(x)\n",
    "        return x\n",
    "        \n",
    "resnet = ResNet50(num_classes=10)\n",
    "model = paddle.Model(resnet)\n",
    "from paddle.static import InputSpec\n",
    "input = InputSpec([None, 1, 28, 28], 'float32', 'image')\n",
    "label = InputSpec([None, 1], 'int64', 'label')\n",
    "model = paddle.Model(resnet, input, label)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1       [[1, 1, 28, 28]]     [1, 64, 14, 14]         3,200     \n",
      " BatchNorm2D-1   [[1, 64, 14, 14]]     [1, 64, 14, 14]          256      \n",
      "    ReLU-1       [[1, 64, 14, 14]]     [1, 64, 14, 14]           0       \n",
      "  MaxPool2D-1    [[1, 64, 14, 14]]      [1, 64, 7, 7]            0       \n",
      "   Conv2D-2       [[1, 64, 7, 7]]       [1, 64, 7, 7]         36,928     \n",
      " BatchNorm2D-2    [[1, 64, 7, 7]]       [1, 64, 7, 7]           256      \n",
      "   Conv2D-3       [[1, 64, 7, 7]]       [1, 64, 7, 7]         36,928     \n",
      " BatchNorm2D-3    [[1, 64, 7, 7]]       [1, 64, 7, 7]           256      \n",
      "  Residual-1      [[1, 64, 7, 7]]       [1, 64, 7, 7]            0       \n",
      "   Conv2D-4       [[1, 64, 7, 7]]       [1, 64, 7, 7]         36,928     \n",
      " BatchNorm2D-4    [[1, 64, 7, 7]]       [1, 64, 7, 7]           256      \n",
      "   Conv2D-5       [[1, 64, 7, 7]]       [1, 64, 7, 7]         36,928     \n",
      " BatchNorm2D-5    [[1, 64, 7, 7]]       [1, 64, 7, 7]           256      \n",
      "  Residual-2      [[1, 64, 7, 7]]       [1, 64, 7, 7]            0       \n",
      "   Conv2D-6       [[1, 64, 7, 7]]       [1, 64, 7, 7]         36,928     \n",
      " BatchNorm2D-6    [[1, 64, 7, 7]]       [1, 64, 7, 7]           256      \n",
      "   Conv2D-7       [[1, 64, 7, 7]]       [1, 64, 7, 7]         36,928     \n",
      " BatchNorm2D-7    [[1, 64, 7, 7]]       [1, 64, 7, 7]           256      \n",
      "  Residual-3      [[1, 64, 7, 7]]       [1, 64, 7, 7]            0       \n",
      "   Conv2D-8       [[1, 64, 7, 7]]       [1, 128, 4, 4]        73,856     \n",
      " BatchNorm2D-8    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "   Conv2D-9       [[1, 128, 4, 4]]      [1, 128, 4, 4]        147,584    \n",
      " BatchNorm2D-9    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "   Conv2D-10      [[1, 64, 7, 7]]       [1, 128, 4, 4]         8,320     \n",
      "  Residual-4      [[1, 64, 7, 7]]       [1, 128, 4, 4]           0       \n",
      "   Conv2D-11      [[1, 128, 4, 4]]      [1, 128, 4, 4]        147,584    \n",
      "BatchNorm2D-10    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "   Conv2D-12      [[1, 128, 4, 4]]      [1, 128, 4, 4]        147,584    \n",
      "BatchNorm2D-11    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "  Residual-5      [[1, 128, 4, 4]]      [1, 128, 4, 4]           0       \n",
      "   Conv2D-13      [[1, 128, 4, 4]]      [1, 128, 4, 4]        147,584    \n",
      "BatchNorm2D-12    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "   Conv2D-14      [[1, 128, 4, 4]]      [1, 128, 4, 4]        147,584    \n",
      "BatchNorm2D-13    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "  Residual-6      [[1, 128, 4, 4]]      [1, 128, 4, 4]           0       \n",
      "   Conv2D-15      [[1, 128, 4, 4]]      [1, 128, 4, 4]        147,584    \n",
      "BatchNorm2D-14    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "   Conv2D-16      [[1, 128, 4, 4]]      [1, 128, 4, 4]        147,584    \n",
      "BatchNorm2D-15    [[1, 128, 4, 4]]      [1, 128, 4, 4]          512      \n",
      "  Residual-7      [[1, 128, 4, 4]]      [1, 128, 4, 4]           0       \n",
      "   Conv2D-17      [[1, 128, 4, 4]]      [1, 256, 2, 2]        295,168    \n",
      "BatchNorm2D-16    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "   Conv2D-18      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-17    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "   Conv2D-19      [[1, 128, 4, 4]]      [1, 256, 2, 2]        33,024     \n",
      "  Residual-8      [[1, 128, 4, 4]]      [1, 256, 2, 2]           0       \n",
      "   Conv2D-20      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-18    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "   Conv2D-21      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-19    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "  Residual-9      [[1, 256, 2, 2]]      [1, 256, 2, 2]           0       \n",
      "   Conv2D-22      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-20    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "   Conv2D-23      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-21    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "  Residual-10     [[1, 256, 2, 2]]      [1, 256, 2, 2]           0       \n",
      "   Conv2D-24      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-22    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "   Conv2D-25      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-23    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "  Residual-11     [[1, 256, 2, 2]]      [1, 256, 2, 2]           0       \n",
      "   Conv2D-26      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-24    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "   Conv2D-27      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-25    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "  Residual-12     [[1, 256, 2, 2]]      [1, 256, 2, 2]           0       \n",
      "   Conv2D-28      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-26    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "   Conv2D-29      [[1, 256, 2, 2]]      [1, 256, 2, 2]        590,080    \n",
      "BatchNorm2D-27    [[1, 256, 2, 2]]      [1, 256, 2, 2]         1,024     \n",
      "  Residual-13     [[1, 256, 2, 2]]      [1, 256, 2, 2]           0       \n",
      "   Conv2D-30      [[1, 256, 2, 2]]      [1, 512, 1, 1]       1,180,160   \n",
      "BatchNorm2D-28    [[1, 512, 1, 1]]      [1, 512, 1, 1]         2,048     \n",
      "   Conv2D-31      [[1, 512, 1, 1]]      [1, 512, 1, 1]       2,359,808   \n",
      "BatchNorm2D-29    [[1, 512, 1, 1]]      [1, 512, 1, 1]         2,048     \n",
      "   Conv2D-32      [[1, 256, 2, 2]]      [1, 512, 1, 1]        131,584    \n",
      "  Residual-14     [[1, 256, 2, 2]]      [1, 512, 1, 1]           0       \n",
      "   Conv2D-33      [[1, 512, 1, 1]]      [1, 512, 1, 1]       2,359,808   \n",
      "BatchNorm2D-30    [[1, 512, 1, 1]]      [1, 512, 1, 1]         2,048     \n",
      "   Conv2D-34      [[1, 512, 1, 1]]      [1, 512, 1, 1]       2,359,808   \n",
      "BatchNorm2D-31    [[1, 512, 1, 1]]      [1, 512, 1, 1]         2,048     \n",
      "  Residual-15     [[1, 512, 1, 1]]      [1, 512, 1, 1]           0       \n",
      "   Conv2D-35      [[1, 512, 1, 1]]      [1, 512, 1, 1]       2,359,808   \n",
      "BatchNorm2D-32    [[1, 512, 1, 1]]      [1, 512, 1, 1]         2,048     \n",
      "   Conv2D-36      [[1, 512, 1, 1]]      [1, 512, 1, 1]       2,359,808   \n",
      "BatchNorm2D-33    [[1, 512, 1, 1]]      [1, 512, 1, 1]         2,048     \n",
      "  Residual-16     [[1, 512, 1, 1]]      [1, 512, 1, 1]           0       \n",
      "  AvgPool2D-1     [[1, 512, 1, 1]]      [1, 512, 1, 1]           0       \n",
      "   Flatten-1      [[1, 512, 1, 1]]         [1, 512]              0       \n",
      "   Linear-1          [[1, 512]]            [1, 10]             5,130     \n",
      "===========================================================================\n",
      "Total params: 21,305,482\n",
      "Trainable params: 21,275,018\n",
      "Non-trainable params: 30,464\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.31\n",
      "Params size (MB): 81.27\n",
      "Estimated Total Size (MB): 82.59\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'total_params': 21305482, 'trainable_params': 21275018}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 模型配置与训练\n",
    "可以使用`Model`搭建实例，然后使用`model.prepare`接口进行模型的配置，比如优化器、损失函数和评价指标等，也可以使用其他方法配置和训练模型。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = paddle.io.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = paddle.io.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "# 为模型训练做准备，设置优化器，损失函数和精度计算方式\n",
    "learning_rate = 0.001\n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "opt = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\n",
    "model.prepare(optimizer=opt, loss=loss_fn, metrics=paddle.metric.Accuracy())\n",
    "\n",
    "log_dir = './log/resnett50'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "callback_train = paddle.callbacks.VisualDL(log_dir=log_dir)\n",
    "\n",
    "# 启动模型训练，指定训练数据集，设置训练轮次，设置每次数据集计算的批次大小，设置日志格式\n",
    "model.fit(train_loader, test_loader, batch_size=64, epochs=20, eval_freq= 5, verbose=1, callbacks=callback_train)\n",
    "model.evaluate(test_loader, verbose=1)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/vincent/.pyenv/versions/3.6.3/envs/torch_3.6.3/lib/python3.6/site-packages/paddle/nn/layer/norm.py:641: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step 938/938 [==============================] - loss: 0.3347 - acc: 0.8354 - 2s/step           \n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 0.3244 - acc: 0.8696 - 835ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 2/20\n",
      "step 540/938 [================>.............] - loss: 0.2648 - acc: 0.8790 - ETA: 15:07 - 2s/step"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save('resnet-50')  # save for inference"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model = paddle.Model(MyNet()) # 用Model封装模型\n",
    "# from paddle.vision.models import LeNet\n",
    "# model = paddle.Model(LeNet())\n",
    "# lr = 0.001\n",
    "# optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "# optimizer = paddle.optimizer.Momentum(learning_rate=lr, parameters=model.parameters(), momentum=0.9)\n",
    "# optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters = model.parameters())\n",
    "# 配置模型\n",
    "# model.prepare(paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()),\n",
    "#               paddle.nn.CrossEntropyLoss(),\n",
    "#               paddle.metric.Accuracy())"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find the best hyperparams\n",
    "# import optuna\n",
    "# from optuna import trial\n",
    "# def objective(trial):\n",
    "    # optimizer = None\n",
    "    # lr = trail.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    # optimizer_name = trail.suggest_categorical(\"optimizer\", [\"optim_Mom\", \"optim_SGD\", 'optim_Adam','optim_Adagrad','optim_RMS'])\n",
    "    # if optimizer_name == 'optim_Mom':\n",
    "    #     optimizer = paddle.optimizer.Momentum(learning_rate=lr, parameters=model.parameters(), momentum=0.9)\n",
    "    # if optimizer_name == 'optim_SGD':\n",
    "    #     optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters = model.parameters())\n",
    "    # if optimizer_name == 'optim_Adam':\n",
    "    #     optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "    # if optimizer_name == 'optim_Adagrad':\n",
    "    #     optimizer = paddle.optimizer.Adagrad(learning_rate=lr, parameters=model.parameters())\n",
    "    # if optimizer_name == 'optim_RMS':\n",
    "    #     optimizer = paddle.optimizer.RMSProp(learning_rate=lr, parameters=model.parameters(), weight_decay=0.01)\n",
    "       # Generate the optimizers.\n",
    "#     optim = paddle.optimizer\n",
    "#     optimizer_name = trial.suggest_categorical(\"optimizer\", ['Momentum', \"Adam\", \"RMSprop\", \"SGD\", 'Adagrad'])\n",
    "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "#     optimizer = getattr(optim, optimizer_name)(parameters=model.parameters(), learning_rate=lr)\n",
    "# study = optuna.create_study()\n",
    "# study.optimize(objective, n_trials=10)\n",
    "# best_params = study.best_params\n",
    "# print(best_params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **作业要求**：完成训练过程可视化的相关函数，训练模型并保存可视化结果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 作业模型运行优化日志：\n",
    "- 第一次 cnn train，learning_rate=0.001, Adam evaluation：0.9 左右，然后再怎么train也不动了\n",
    "- 第二次 使用默认的LeNet，lr = 0.001, Adam evaluation: {'loss': [0.7828319], 'acc': 0.8948}\n",
    "- 第三次 使用lenet Momentum lr=0.001, evaluation: {'loss': [0.1281017], 'acc': 0.8981}\n",
    "- 第四次 使用mynet Momentum lr=0.001, {'loss': [0.39709407], 'acc': 0.9044}\n",
    "- 第五次 使用mynet SGD lr=0.001：{'loss': [0.2486805], 'acc': 0.892}\n",
    "- 第六次 使用mynet SGD lr=0.01： {'loss': [4.3855166], 'acc': 0.8912}\n",
    "- 第七次 使用mynet Momentum lr=0.01：{'loss': [0.011295634], 'acc': 0.9005}\n",
    "- 尝试使用 optuna 来寻找最优参数, 最终发现不能够使用啊，paddle 支持"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# 参数的分布图， 损失函数， acc\n",
    "def DrawLossFunction():\n",
    "    pass\n",
    "log_dir = './log/resnet18'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "callback_train = paddle.callbacks.VisualDL(log_dir=log_dir)\n",
    "\n",
    "model.fit(train_dataset, test_dataset, epochs=50, batch_size=64, verbose=1, callbacks=callback_train)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.evaluate(test_dataset, verbose=1) # batch_size=64,"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.3 64-bit ('torch_3.6.3': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "interpreter": {
   "hash": "328dc1e95799e67618d905377f310fe5aa234fc2f08c1eab05fb9c8357706382"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}