{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业07：CV中的Transformer与迁移学习\n",
    "\n",
    "## 1. 作业要求\n",
    "1. 了解ViT的原理和网络结构，将ImageNet上预训练的ViT模型迁移至美食分类问题上。\n",
    "2. 在README文件中描述具体的迁移学习方法。\n",
    "3. 可视化模型迁移学习过程中损失函数与准确率的变化趋势，并保存可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入环境\r\n",
    "import os\r\n",
    "import zipfile\r\n",
    "import random\r\n",
    "import json\r\n",
    "import sys\r\n",
    "import numpy as np\r\n",
    "import cv2\r\n",
    "from PIL import Image\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import paddle\r\n",
    "from paddle.io import Dataset\r\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, Dropout, BatchNorm, AdaptiveAvgPool2D, AvgPool2D\r\n",
    "import paddle.nn.functional as F\r\n",
    "import paddle.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. ViT\n",
    "\n",
    "### 2.1 ViT算法综述\n",
    "论文地址：[An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "之前的算法大都是保持CNN整体结构不变，在CNN中增加attention模块或者使用attention模块替换CNN中的某些部分。ViT算法中，作者提出没有必要总是依赖于CNN，仅仅使用Transformer结构也能够在图像分类任务中表现很好。\n",
    "\n",
    "受到NLP领域中Transformer成功应用的启发，ViT算法中尝试将标准的Transformer结构直接应用于图像，并对整个图像分类流程进行最少的修改。具体来讲，ViT算法中，会将整幅图像拆分成小图像块，然后把这些小图像块的线性嵌入序列作为Transformer的输入送入网络，然后使用监督学习的方式进行图像分类的训练。ViT算法的整体结构如 **图1** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5d33d430cbfe43cb98c6c9926618cb2da8e52318a00341da92b83bc32bedeabb\" width = \"800\"></center>\n",
    "<center><br>图1：ViT算法结构示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "该算法在中等规模（例如ImageNet）以及大规模（例如ImageNet-21K、JFT-300M）数据集上进行了实验验证，发现：\n",
    "* Tranformer相较于CNN结构，缺少一定的平移不变性和局部感知性，因此在数据量不充分时，很难达到同等的效果。具体表现为使用中等规模的ImageNet训练的Tranformer会比ResNet在精度上低几个百分点。\n",
    "* 当有大量的训练样本时，结果则会发生改变。使用大规模数据集进行预训练后，再使用迁移学习的方式应用到其他数据集上，可以达到或超越当前的SOTA水平。\n",
    "\n",
    "**图2** 为大家展示了使用大规模数据集预训练后的 ViT 算法，迁移到其他小规模数据集进行训练，与使用 CNN 结构的SOTA算法精度对比。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/85d8e641c74648b3b0f34033d137b4ca0f8d9dc7e4a24eeba40e0da48dec96c7\" width = \"800\"></center>\n",
    "<center><br>图2：ViT模型精度对比</br></center>\n",
    "<br></br>\n",
    "\n",
    "图中前3列为不同尺度的ViT模型，使用不同的大规模数据集进行预训练，并迁移到各个子任务上的结果。第4列为BiT算法基于JFT-300M数据集预训练后，迁移到各个子任务的结果。第5列为2020年提出的半监督算法 Noisy Student 在 ImageNet 和 ImageNet ReaL 数据集上的结果。\n",
    "\n",
    "---\n",
    "\n",
    "**说明：**\n",
    "\n",
    "BiT 与 Noisy Student 均为2020年提出的 SOTA 算法。\n",
    "\n",
    "BiT算法：使用大规模数据集 JFT-300M 对 ResNet 结构进行预训练，其中，作者发现模型越大，预训练效果越好，最终指标最高的为4倍宽、152层深的 $ResNet152 \\times 4$。论文地址：[Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)\n",
    "\n",
    "Noisy Student 算法：使用知识蒸馏的技术，基于 EfficientNet 结构，利用未标签数据，提高训练精度。论文地址：[Self-training with Noisy Student improves ImageNet classification](https://arxiv.org/abs/1911.04252)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "接下来，分别看一下ViT算法的各个组成部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 图像分块嵌入\n",
    "\n",
    "考虑到之前课程中学习的，Transformer结构中，输入需要是一个二维的矩阵，矩阵的形状可以表示为 $(N,D)$，其中 $N$ 是sequence的长度，而 $D$ 是sequence中每个向量的维度。因此，在ViT算法中，首先需要设法将 $H \\times W \\times C$ 的三维图像转化为 $(N,D)$ 的二维输入。\n",
    "\n",
    "ViT中的具体实现方式为：将 $H \\times W \\times C$ 的图像，变为一个 $N \\times (P^2 * C)$ 的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了 $N=HW/P^2$ 个图像块，每个图像块的维度则是 $(P^2*C)$。其中  $P$ 是图像块的大小，$C$ 是通道数量。经过如上变换，就可以将 $N$ 视为sequence的长度了。\n",
    "\n",
    "但是，此时每个图像块的维度是 $(P^2*C)$，而我们实际需要的向量维度是 $D$，因此我们还需要对图像块进行 Embedding。这里 Embedding 的方式非常简单，只需要对每个 $(P^2*C)$ 的图像块做一个线性变换，将维度压缩为 $D$ 即可。\n",
    "\n",
    "上述对图像进行分块以及 Embedding 的具体方式如 **图3** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/a1dbbb5ad2384df88f24fc739836c31f42bf2056f78c491cbc5d31b78b933ee3\" width = \"800\"></center>\n",
    "<center><br>图3：图像分块嵌入示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "具体代码实现如下所示。其中，使用了大小为 $P$ 的卷积来代替对每个大小为 $P$ 图像块展平后使用全连接进行运算的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 图像分块嵌入模块\n",
    "class PatchEmbed(nn.Layer):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        # 原始大小为int，转为tuple，即：img_size原始输入224，变换后为[224,224]\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        # 图像块的个数\n",
    "        num_patches = (img_size[1] // patch_size[1]) * \\\n",
    "            (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        # kernel_size=块大小，即每个块输出一个值，类似每个块展平后使用相同的全连接层进行处理\n",
    "        # 输入维度为3，输出维度为块向量长度\n",
    "        # 与原文中：分块、展平、全连接降维保持一致\n",
    "        # 输出为[B, C, H, W]\n",
    "        self.proj = nn.Conv2D(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            \"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        # [B, C, H, W] -> [B, C, H*W] ->[B, H*W, C]\n",
    "        x = self.proj(x).flatten(2).transpose((0, 2, 1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 多头注意力\n",
    "\n",
    "将图像转化为 $N \\times (P^2 * C)$ 的序列后，就可以将其输入到 Tranformer 结构中进行特征提取了。在前面的课程中，我们了解到 Tranformer 结构中最重要的结构就是 Multi-head Attention，即多头注意力结构，如 **图4** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8566c2480d554506be0c83eb0a0a60736d26aa23b23246bf8db88d59b21a55c9\" width = \"800\"></center>\n",
    "<center><br>图4：Multi-head Attention 示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "具有2个head的 Multi-head Attention 结构如 **图5** 所示。输入 $a^i$ 经过转移矩阵，并切分生成 $q^{(i,1)}$、$q^{(i,2)}$、$k^{(i,1)}$、$k^{(i,2)}$、$v^{(i,1)}$、$v^{(i,2)}$，然后 $q^{(i,1)}$ 与 $k^{(i,1)}$ 做 attention，得到权重向量 $\\alpha$，将 $\\alpha$ 与 $v^{(i,1)}$ 进行加权求和，得到最终的 $b^{(i,1)}(i=1,2,…,N)$，同理可以得到 $b^{(i,2)}(i=1,2,…,N)$。接着将它们拼接起来，通过一个线性层进行处理，得到最终的结果。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4953243f18af450eae3d16181b9a77ce83f4623e414747298b0d7c056c3a6bfe\" width = \"800\"></center>\n",
    "<center><br>图5：Multi-head Attention结构</br></center>\n",
    "<br></br>\n",
    "\n",
    "其中，使用 $q^{(i,j)}$、$k^{(i,j)}$ 与 $v^{(i,j)}$ 计算 $b^{(i,j)}(i=1,2,…,N)$ 的方法是 Scaled Dot-Product Attention。 结构如 **图6** 所示。首先使用每个 $q^{(i,j)}$ 去与 $k^{(i,j)}$ 做 attention，这里说的 attention 就是匹配这两个向量有多接近，具体的方式就是计算向量的加权内积，得到 $\\alpha_{(i,j)}$。这里的加权内积计算方式如下所示：\n",
    "\n",
    "$$ \\alpha_{(1,i)} =  q^1 * k^i / \\sqrt{d} $$\n",
    "\n",
    "其中，$d$ 是 $q$ 和 $k$ 的维度，因为 $q*k$ 的数值会随着维度的增大而增大，因此除以 $\\sqrt{d}$ 的值也就相当于归一化的效果。\n",
    "\n",
    "接下来，把计算得到的 $\\alpha_{(i,j)}$ 取 softmax 操作，再将其与 $v^{(i,j)}$ 相乘。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5b3da7158a92461aa1f5cd0bd294a9aba0935bf02d74461b9aa15d48784e8f4e\" width = \"400\"></center>\n",
    "<center><br>图6：Scaled Dot-Product Attention</br></center>\n",
    "<br></br>\n",
    "\n",
    "**想了解注意力机制的更多信息，请参阅[awesome-DeepLearning](https://github.com/paddlepaddle/awesome-DeepLearning) 中的 [注意力机制知识点](https://github.com/PaddlePaddle/awesome-DeepLearning/tree/master/docs/tutorials/deep_learning/model_tuning/attention)。**\n",
    "\n",
    "具体代码实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multi-head Attention\n",
    "class Attention(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "        # 计算 q,k,v 的转移矩阵\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        # 最终的线性层\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C = x.shape[1:]\n",
    "        # 线性变换\n",
    "        qkv = self.qkv(x).reshape((-1, N, 3, self.num_heads, C //\n",
    "                                   self.num_heads)).transpose((2, 0, 3, 1, 4))\n",
    "        # 分割 query key value\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        # Scaled Dot-Product Attention\n",
    "        # Matmul + Scale\n",
    "        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\n",
    "        # SoftMax\n",
    "        attn = nn.functional.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        # Matmul\n",
    "        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, N, C))\n",
    "        # 线性变换\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 多层感知机（MLP）\n",
    "\n",
    " Tranformer 结构中还有一个重要的结构就是 MLP，即多层感知机，如 **图7** 所示。\n",
    " \n",
    " <center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/62a1efbf38bb4c119e89cf277dc2653394a19af9cea5476182406a2ebc0572e9\" width = \"600\"></center>\n",
    "<center><br>图7：多层感知机</br></center>\n",
    "<br></br>\n",
    " \n",
    " 多层感知机由输入层、输出层和至少一层的隐藏层构成。网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。在多层感知机中，相邻层所包含的神经元之间通常使用“全连接”方式进行连接。多层感知机可以模拟复杂非线性函数功能，所模拟函数的复杂性取决于网络隐藏层数目和各层中神经元数目。多层感知机的结构如 **图8** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/9ada33e2b5134412b2b3dd04dfc0e6e88e932555045147ce99a880f06d69db23\" width = \"400\"></center>\n",
    "<center><br>图8：多层感知机结构</br></center>\n",
    "<br></br>\n",
    "\n",
    "**想了解多层感知机的更多信息，请参阅[awesome-DeepLearning](https://github.com/paddlepaddle/awesome-DeepLearning) 中的 [多层感知机知识点](https://github.com/PaddlePaddle/awesome-DeepLearning/blob/master/docs/tutorials/deep_learning/basic_concepts/multilayer_perceptron.md)。**\n",
    "\n",
    "具体代码实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 多层感知机\n",
    "class Mlp(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=nn.GELU,\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入层：线性变换\n",
    "        x = self.fc1(x)\n",
    "        # 应用激活函数\n",
    "        x = self.act(x)\n",
    "        # Dropout\n",
    "        x = self.drop(x)\n",
    "        # 输出层：线性变换\n",
    "        x = self.fc2(x)\n",
    "        # Dropout\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.5 基础模块\n",
    "\n",
    "基于上面实现的 Attention、MLP 和 DropPath 模块就可以组合出 Vision Transformer 模型的一个基础模块，如 **图9** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5f8f09402d5d442c8f357aa39912865c2253cc7eec374d52821d7f35e566ca67\" width = \"600\"></center>\n",
    "<center><br>图9：Transformer 基础模块</br></center>\n",
    "<br></br>\n",
    "\n",
    "ViT使用了DropPath（Stochastic Depth）来代替传统的Dropout结构，DropPath可以理解为一种特殊的 Dropout。其作用是在训练过程中随机丢弃子图层（randomly drop a subset of layers），而在预测时正常使用完整的 Graph。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob=0., training=False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = paddle.to_tensor(1 - drop_prob)\n",
    "    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\n",
    "    random_tensor = paddle.floor(random_tensor)\n",
    "    output = x.divide(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Layer):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "# 基础模块\n",
    "class Block(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer='nn.LayerNorm',\n",
    "                 epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n",
    "        # Multi-head Self-attention\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop)\n",
    "        # DropPath\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n",
    "        self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim,\n",
    "                       hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head Self-attention， Add， LayerNorm\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        # Feed Forward， Add， LayerNorm\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.6 定义ViT网络\n",
    "\n",
    "基础模块构建好后，就可以构建完整的ViT网络了。ViT的完整结构如 **图10** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/60f51da9f9dc477182c9c107d27867b743ff2dcee5fe427fbf81a9d5c0a01806\" width = \"600\"></center>\n",
    "<center><br>图10：ViT网络结构</br></center>\n",
    "<br></br>\n",
    "\n",
    "在实现完整网络结构之前，还需要给大家介绍几个模块：\n",
    "\n",
    "1. Class Token\n",
    "\n",
    "可以看到，假设我们将原始图像切分成 $3 \\times 3$ 共9个小图像块，最终的输入序列长度却是10，也就是说我们这里人为的增加了一个向量进行输入，我们通常将人为增加的这个向量称为 Class Token。那么这个 Class Token 有什么作用呢？\n",
    "\n",
    "我们可以想象，如果没有这个向量，也就是将 $N=9$ 个向量输入 Transformer 结构中进行编码，我们最终会得到9个编码向量，可对于图像分类任务而言，我们应该选择哪个输出向量进行后续分类呢？\n",
    "\n",
    "由于选择9个中的哪个都不合适，所以ViT算法中，提出了一个可学习的嵌入向量 Class Token，将它与9个向量一起输入到 Transformer 结构中，输出10个编码向量，然后用这个 Class Token 进行分类预测即可。\n",
    "\n",
    "其实这里也可以理解为：ViT 其实只用到了 Transformer 中的 Encoder，而并没有用到 Decoder，而 Class Token 的作用就是寻找其他9个输入向量对应的类别。\n",
    "\n",
    "2. Positional Encoding\n",
    "\n",
    "按照 Transformer 结构中的位置编码习惯，这个工作也使用了位置编码。不同的是，ViT 中的位置编码没有采用原版 Transformer 中的 $sincos$ 编码，而是直接设置为可学习的 Positional Encoding。对训练好的 Positional Encoding 进行可视化，如 **图11** 所示。我们可以看到，位置越接近，往往具有更相似的位置编码。此外，出现了行列结构，同一行/列中的 patch 具有相似的位置编码。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3c2889396cab4790bffb2c23b0954fe552411b45dce14a12a925e2f3ee164790\" width = \"600\"></center>\n",
    "<center><br>图11：Positional Encoding </br></center>\n",
    "<br></br>\n",
    "\n",
    "3. MLP Head\n",
    "\n",
    "得到输出后，ViT中使用了 MLP Head对输出进行分类处理，这里的 MLP Head 由 LayerNorm 和两层全连接层组成，并且采用了 GELU 激活函数。\n",
    "\n",
    "具体代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 参数初始化配置\r\n",
    "trunc_normal_ = nn.initializer.TruncatedNormal(std=.02)\r\n",
    "zeros_ = nn.initializer.Constant(value=0.)\r\n",
    "ones_ = nn.initializer.Constant(value=1.)\r\n",
    "\r\n",
    "# 将输入 x 由 int 类型转为 tuple 类型\r\n",
    "def to_2tuple(x):\r\n",
    "    return tuple([x] * 2)\r\n",
    "\r\n",
    "# 定义一个什么操作都不进行的网络层\r\n",
    "class Identity(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(Identity, self).__init__()\r\n",
    "\r\n",
    "    def forward(self, input):\r\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 img_size=384,\r\n",
    "                 patch_size=16,\r\n",
    "                 in_chans=3,\r\n",
    "                 class_dim=1000,\r\n",
    "                 embed_dim=768,\r\n",
    "                 depth=12,\r\n",
    "                 num_heads=12,\r\n",
    "                 mlp_ratio=4,\r\n",
    "                 qkv_bias=False,\r\n",
    "                 qk_scale=None,\r\n",
    "                 drop_rate=0.,\r\n",
    "                 attn_drop_rate=0.,\r\n",
    "                 drop_path_rate=0.,\r\n",
    "                 norm_layer='nn.LayerNorm',\r\n",
    "                 epsilon=1e-5,\r\n",
    "                 **args):\r\n",
    "        super().__init__()\r\n",
    "        self.class_dim = class_dim\r\n",
    "\r\n",
    "        self.num_features = self.embed_dim = embed_dim\r\n",
    "        # 图片分块和降维，块大小为patch_size，最终块向量维度为768\r\n",
    "        self.patch_embed = PatchEmbed(\r\n",
    "            img_size=img_size,\r\n",
    "            patch_size=patch_size,\r\n",
    "            in_chans=in_chans,\r\n",
    "            embed_dim=embed_dim)\r\n",
    "        # 分块数量\r\n",
    "        num_patches = self.patch_embed.num_patches\r\n",
    "        # 可学习的位置编码\r\n",
    "        self.pos_embed = self.create_parameter(\r\n",
    "            shape=(1, num_patches + 1, embed_dim), default_initializer=zeros_)\r\n",
    "        self.add_parameter(\"pos_embed\", self.pos_embed)\r\n",
    "        # 人为追加class token，并使用该向量进行分类预测\r\n",
    "        self.cls_token = self.create_parameter(\r\n",
    "            shape=(1, 1, embed_dim), default_initializer=zeros_)\r\n",
    "        self.add_parameter(\"cls_token\", self.cls_token)\r\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\r\n",
    "\r\n",
    "        dpr = np.linspace(0, drop_path_rate, depth)\r\n",
    "        # transformer\r\n",
    "        self.blocks = nn.LayerList([\r\n",
    "            Block(\r\n",
    "                dim=embed_dim,\r\n",
    "                num_heads=num_heads,\r\n",
    "                mlp_ratio=mlp_ratio,\r\n",
    "                qkv_bias=qkv_bias,\r\n",
    "                qk_scale=qk_scale,\r\n",
    "                drop=drop_rate,\r\n",
    "                attn_drop=attn_drop_rate,\r\n",
    "                drop_path=dpr[i],\r\n",
    "                norm_layer=norm_layer,\r\n",
    "                epsilon=epsilon) for i in range(depth)\r\n",
    "        ])\r\n",
    "\r\n",
    "        self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\r\n",
    "\r\n",
    "        # Classifier head ## 加几层\r\n",
    "        self.head = nn.Linear(embed_dim,\r\n",
    "                              512)\r\n",
    "        self.head2 = nn.Linear(512, class_dim) if class_dim > 0 else Identity()\r\n",
    "\r\n",
    "        trunc_normal_(self.pos_embed)\r\n",
    "        trunc_normal_(self.cls_token)\r\n",
    "        self.apply(self._init_weights)\r\n",
    "    # 参数初始化\r\n",
    "    def _init_weights(self, m):\r\n",
    "        if isinstance(m, nn.Linear):\r\n",
    "            trunc_normal_(m.weight)\r\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\r\n",
    "                zeros_(m.bias)\r\n",
    "        elif isinstance(m, nn.LayerNorm):\r\n",
    "            zeros_(m.bias)\r\n",
    "            ones_(m.weight)\r\n",
    "    # 获取图像特征\r\n",
    "    def forward_features(self, x):\r\n",
    "        B = paddle.shape(x)[0]\r\n",
    "        # 将图片分块，并调整每个块向量的维度\r\n",
    "        x = self.patch_embed(x)\r\n",
    "        # 将class token与前面的分块进行拼接\r\n",
    "        cls_tokens = self.cls_token.expand((B, -1, -1))\r\n",
    "        x = paddle.concat((cls_tokens, x), axis=1)\r\n",
    "        # 将编码向量中加入位置编码\r\n",
    "        x = x + self.pos_embed\r\n",
    "        x = self.pos_drop(x)\r\n",
    "        # 堆叠 transformer 结构\r\n",
    "        for blk in self.blocks:\r\n",
    "            x = blk(x)\r\n",
    "        # LayerNorm\r\n",
    "        x = self.norm(x)\r\n",
    "        # 提取分类 tokens 的输出\r\n",
    "        return x[:, 0]\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # 获取图像特征\r\n",
    "        x = self.forward_features(x)\r\n",
    "        # 图像分类\r\n",
    "        x.stop_gradient=True # 前面的层不进行训练，即只训练最后一层：\r\n",
    "        x = self.head(x)\r\n",
    "        x = self.head2(x)\r\n",
    "\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 基于ImageNet预训练模型的美食分类\n",
    "\n",
    "在上文中，已经详细介绍了ViT的算法原理，以及如何使用Paddle实现ViT的模型结构。这一部分为美食分类数据集的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\r\n",
    "参数配置\r\n",
    "'''\r\n",
    "train_parameters = {\r\n",
    "    \"input_size\": [3, 64, 64],                                #输入图片的shape\r\n",
    "    \"class_dim\": -1,                                          #分类数\r\n",
    "    \"src_path\":\"data/data42610/foods.zip\",                    #原始数据集路径\r\n",
    "    \"target_path\":\"/home/aistudio/data/\",                     #要解压的路径\r\n",
    "    \"train_list_path\": \"/home/aistudio/data/train.txt\",       #train.txt路径\r\n",
    "    \"eval_list_path\": \"/home/aistudio/data/eval.txt\",         #eval.txt路径\r\n",
    "    \"readme_path\": \"/home/aistudio/data/readme.json\",         #readme.json路径\r\n",
    "    \"label_dict\":{},                                          #标签字典\r\n",
    "    \"num_epochs\": 15,                                          #训练轮数\r\n",
    "    \"train_batch_size\": 16,                                   #训练时每个批次的大小\r\n",
    "    \"learning_strategy\": {                                    #优化函数相关的配置\r\n",
    "        \"lr\": 0.01                                          #超参数学习率\r\n",
    "    } \r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 数据准备\n",
    "\n",
    "解压模型权重文件。这里使用的模型为PaddleClas套件中预训练的[ViT_base_\n",
    "patch16_384](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/ViT_base_patch16_384_pretrained.pdparams)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压权重文件\r\n",
    "# !unzip -q -o /home/aistudio/data/data105741/pretrained.zip -d /home/aistudio/work/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 图像预处理与批量读取\n",
    "对美食分类的图像进行预处理，使其满足ViT网络的要求：\n",
    "1. 图像以Numpy格式存储；\n",
    "2. 长宽统一裁剪/缩放为384x384\n",
    "3. **归一化**：将神经网络每层中任意神经元的输入值分布归一化为标准正态分布\n",
    "4. **通道变换**：图像的数据格式为[H, W, C]（即高度、宽度和通道数），而神经网络使用的训练数据的格式为[C, H, W]，因此需要对图像数据重新排列，例如[384, 384, 3]变为[3, 384, 384]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unzip_data(src_path,target_path):\r\n",
    "    '''\r\n",
    "    解压原始数据集，将src_path路径下的zip包解压至target_path目录下\r\n",
    "    '''\r\n",
    "    if(not os.path.isdir(target_path + \"foods\")):     \r\n",
    "        z = zipfile.ZipFile(src_path, 'r')\r\n",
    "        z.extractall(path=target_path)\r\n",
    "        z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成数据列表完成！\n"
     ]
    }
   ],
   "source": [
    "def get_data_list(target_path,train_list_path,eval_list_path):\r\n",
    "    '''\r\n",
    "    生成数据列表\r\n",
    "    '''\r\n",
    "    #存放所有类别的信息\r\n",
    "    class_detail = []\r\n",
    "    #获取所有类别保存的文件夹名称\r\n",
    "    data_list_path=target_path+\"foods/\"\r\n",
    "    class_dirs = os.listdir(data_list_path)  \r\n",
    "    #总的图像数量\r\n",
    "    all_class_images = 0\r\n",
    "    #存放类别标签\r\n",
    "    class_label=0\r\n",
    "    #存放类别数目\r\n",
    "    class_dim = 0\r\n",
    "    #存储要写进eval.txt和train.txt中的内容\r\n",
    "    trainer_list=[]\r\n",
    "    eval_list=[]\r\n",
    "    #读取每个类别\r\n",
    "    for class_dir in class_dirs:\r\n",
    "        if class_dir != \".DS_Store\":\r\n",
    "            class_dim += 1\r\n",
    "            #每个类别的信息\r\n",
    "            class_detail_list = {}\r\n",
    "            eval_sum = 0\r\n",
    "            trainer_sum = 0\r\n",
    "            #统计每个类别有多少张图片\r\n",
    "            class_sum = 0\r\n",
    "            #获取类别路径 \r\n",
    "            path = data_list_path  + class_dir\r\n",
    "            # 获取所有图片\r\n",
    "            img_paths = os.listdir(path)\r\n",
    "            for img_path in img_paths:                                  # 遍历文件夹下的每个图片\r\n",
    "                name_path = path + '/' + img_path                       # 每张图片的路径\r\n",
    "                if class_sum % 8 == 0:                                  # 每8张图片取一个做验证数据\r\n",
    "                    eval_sum += 1                                       # test_sum为测试数据的数目\r\n",
    "                    eval_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")\r\n",
    "                else:\r\n",
    "                    trainer_sum += 1 \r\n",
    "                    trainer_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")#trainer_sum测试数据的数目\r\n",
    "                class_sum += 1                                          #每类图片的数目\r\n",
    "                all_class_images += 1                                   #所有类图片的数目\r\n",
    "             \r\n",
    "            # 说明的json文件的class_detail数据\r\n",
    "            class_detail_list['class_name'] = class_dir             #类别名称\r\n",
    "            class_detail_list['class_label'] = class_label          #类别标签\r\n",
    "            class_detail_list['class_eval_images'] = eval_sum       #该类数据的测试集数目\r\n",
    "            class_detail_list['class_trainer_images'] = trainer_sum #该类数据的训练集数目\r\n",
    "            class_detail.append(class_detail_list)  \r\n",
    "            #初始化标签列表\r\n",
    "            train_parameters['label_dict'][str(class_label)] = class_dir\r\n",
    "            class_label += 1 \r\n",
    "            \r\n",
    "    #初始化分类数\r\n",
    "    train_parameters['class_dim'] = class_dim\r\n",
    "    \r\n",
    "    #乱序  \r\n",
    "    random.shuffle(eval_list)\r\n",
    "    with open(eval_list_path, 'a') as f:\r\n",
    "        for eval_image in eval_list:\r\n",
    "            f.write(eval_image) \r\n",
    "            \r\n",
    "    random.shuffle(trainer_list)\r\n",
    "    with open(train_list_path, 'a') as f2:\r\n",
    "        for train_image in trainer_list:\r\n",
    "            f2.write(train_image) \r\n",
    "\r\n",
    "    # 说明的json文件信息\r\n",
    "    readjson = {}\r\n",
    "    readjson['all_class_name'] = data_list_path                  #文件父目录\r\n",
    "    readjson['all_class_images'] = all_class_images\r\n",
    "    readjson['class_detail'] = class_detail\r\n",
    "    jsons = json.dumps(readjson, sort_keys=True, indent=4, separators=(',', ': '))\r\n",
    "    with open(train_parameters['readme_path'],'w') as f:\r\n",
    "        f.write(jsons)\r\n",
    "    print ('生成数据列表完成！')\r\n",
    "\r\n",
    "'''\r\n",
    "参数初始化\r\n",
    "'''\r\n",
    "src_path=train_parameters['src_path']\r\n",
    "target_path=train_parameters['target_path']\r\n",
    "train_list_path=train_parameters['train_list_path']\r\n",
    "eval_list_path=train_parameters['eval_list_path']\r\n",
    "batch_size=train_parameters['train_batch_size']\r\n",
    "\r\n",
    "'''\r\n",
    "解压原始数据到指定路径\r\n",
    "'''\r\n",
    "unzip_data(src_path,target_path)\r\n",
    "\r\n",
    "'''\r\n",
    "划分训练集与验证集，乱序，生成数据列表\r\n",
    "'''\r\n",
    "#每次生成数据列表前，首先清空train.txt和eval.txt\r\n",
    "with open(train_list_path, 'w') as f: \r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "with open(eval_list_path, 'w') as f: \r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "    \r\n",
    "#生成数据列表   \r\n",
    "get_data_list(target_path,train_list_path,eval_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddle.vision.transforms as T\r\n",
    "import numpy as np\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "\r\n",
    "class FoodDataset(paddle.io.Dataset):\r\n",
    "    \"\"\"\r\n",
    "    5类food数据集类的定义\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        初始化函数\r\n",
    "        \"\"\"\r\n",
    "        self.data = []\r\n",
    "        with open('data/{}.txt'.format(mode)) as f:\r\n",
    "            for line in f.readlines():\r\n",
    "                info = line.strip().split('\\t')\r\n",
    "                if len(info) > 0:\r\n",
    "                    self.data.append([info[0].strip(), info[1].strip()])\r\n",
    "        self.transforms = T.Compose([\r\n",
    "            T.Resize((384, 384)),    # 图片缩放\r\n",
    "            T.ToTensor(),                       # 数据的格式转换和标准化、 HWC => CHW            \r\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
    "            ])        \r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        根据索引获取单个样本\r\n",
    "        \"\"\"\r\n",
    "        image_file, label = self.data[index]\r\n",
    "        image = Image.open(image_file)\r\n",
    "        if image.mode != 'RGB':\r\n",
    "            image = image.convert('RGB')\r\n",
    "        image = self.transforms(image)\r\n",
    "        return image, np.array(label, dtype='int64')\r\n",
    "    def __len__(self):\r\n",
    "        \"\"\"\r\n",
    "        获取样本总数\r\n",
    "        \"\"\"\r\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375\n",
      "625\n"
     ]
    }
   ],
   "source": [
    "'''\r\n",
    "构造数据提供器\r\n",
    "'''\r\n",
    "train_dataset = FoodDataset(mode='train')\r\n",
    "eval_dataset = FoodDataset(mode='eval')\r\n",
    "# train_dataload = \r\n",
    "batch_size = train_parameters['train_batch_size']\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, return_list=True, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "test_loader = paddle.io.DataLoader(eval_dataset, return_list=True, shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "print(train_dataset.__len__())\r\n",
    "print(eval_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. 迁移学习\n",
    "\n",
    "将预训练的ViT模型迁移至美食分类问题上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.1 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1204 12:55:25.674160   120 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W1204 12:55:25.679523   120 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1436: UserWarning: Skip loading for head.weight. head.weight receives a shape [768, 1000], but the expected shape is [768, 512].\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1436: UserWarning: Skip loading for head.bias. head.bias receives a shape [1000], but the expected shape is [512].\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1436: UserWarning: Skip loading for head2.weight. head2.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1436: UserWarning: Skip loading for head2.bias. head2.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\r\n",
    "model = VisionTransformer(\r\n",
    "        patch_size=16,\r\n",
    "        class_dim=5,\r\n",
    "        embed_dim=768,\r\n",
    "        depth=12,\r\n",
    "        num_heads=12,\r\n",
    "        mlp_ratio=4,\r\n",
    "        qkv_bias=True,\r\n",
    "        epsilon=1e-6)\r\n",
    "\r\n",
    "# 加载模型参数\r\n",
    "params_file_path=\"/home/aistudio/work/ViT_base_patch16_384_pretrained.pdparams\"\r\n",
    "model_state_dict = paddle.load(params_file_path)\r\n",
    "model.load_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.2 调整模型超参数 & 4.3 Fine Tuning\n",
    "\n",
    "调整模型最后的全连接层以适应美食分类任务（5分类），并完成对模型Fine Tuning的代码。\n",
    "\n",
    "**参考资料**：\n",
    "1. [动态图/静态图转换](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/04_dygraph_to_static/index_cn.html#dongtaituzhuanjingtaitu)与[冻结静态图参数](https://www.paddlepaddle.org.cn/documentation/docs/zh/faq/train_cn.html#stop-gradient-true)。\n",
    "2. [PaddleHub](https://www.paddlepaddle.org.cn/hub)以及[模型打包](https://aistudio.baidu.com/aistudio/projectdetail/1259178)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:130: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/273 - loss: 2.3835 - acc: 0.6188 - 302ms/step\n",
      "step  20/273 - loss: 1.4407 - acc: 0.7031 - 287ms/step\n",
      "step  30/273 - loss: 4.2922 - acc: 0.7521 - 282ms/step\n",
      "step  40/273 - loss: 5.5073 - acc: 0.7734 - 279ms/step\n",
      "step  50/273 - loss: 8.4710 - acc: 0.7812 - 278ms/step\n",
      "step  60/273 - loss: 1.8887 - acc: 0.7802 - 277ms/step\n",
      "step  70/273 - loss: 3.4805 - acc: 0.7875 - 276ms/step\n",
      "step  80/273 - loss: 1.0708 - acc: 0.7945 - 276ms/step\n",
      "step  90/273 - loss: 4.2988e-06 - acc: 0.8007 - 275ms/step\n",
      "step 100/273 - loss: 3.2068 - acc: 0.8050 - 275ms/step\n",
      "step 110/273 - loss: 0.6320 - acc: 0.8136 - 275ms/step\n",
      "step 120/273 - loss: 1.0373 - acc: 0.8146 - 275ms/step\n",
      "step 130/273 - loss: 1.8626e-07 - acc: 0.8144 - 275ms/step\n",
      "step 140/273 - loss: 5.5921 - acc: 0.8161 - 275ms/step\n",
      "step 150/273 - loss: 1.7276 - acc: 0.8217 - 275ms/step\n",
      "step 160/273 - loss: 1.1744 - acc: 0.8230 - 274ms/step\n",
      "step 170/273 - loss: 4.3958e-07 - acc: 0.8239 - 274ms/step\n",
      "step 180/273 - loss: 4.0285 - acc: 0.8260 - 274ms/step\n",
      "step 190/273 - loss: 3.4025 - acc: 0.8270 - 274ms/step\n",
      "step 200/273 - loss: 6.5563 - acc: 0.8272 - 274ms/step\n",
      "step 210/273 - loss: 3.9085 - acc: 0.8277 - 274ms/step\n",
      "step 220/273 - loss: 0.0000e+00 - acc: 0.8295 - 274ms/step\n",
      "step 230/273 - loss: 8.3373 - acc: 0.8321 - 274ms/step\n",
      "step 240/273 - loss: 0.1804 - acc: 0.8336 - 274ms/step\n",
      "step 250/273 - loss: 4.1868 - acc: 0.8345 - 274ms/step\n",
      "step 260/273 - loss: 9.8270 - acc: 0.8351 - 274ms/step\n",
      "step 270/273 - loss: 8.3193 - acc: 0.8359 - 274ms/step\n",
      "step 273/273 - loss: 7.0367 - acc: 0.8365 - 274ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 3.5183 - acc: 0.8562 - 279ms/step\n",
      "step 20/39 - loss: 5.7782 - acc: 0.8844 - 277ms/step\n",
      "step 30/39 - loss: 7.0218 - acc: 0.8896 - 277ms/step\n",
      "step 39/39 - loss: 6.1914 - acc: 0.8910 - 276ms/step\n",
      "Eval samples: 624\n",
      "Epoch 2/15\n",
      "step  10/273 - loss: 0.2035 - acc: 0.9187 - 274ms/step\n",
      "step  20/273 - loss: 2.1226 - acc: 0.8969 - 274ms/step\n",
      "step  30/273 - loss: 3.5317 - acc: 0.9083 - 273ms/step\n",
      "step  40/273 - loss: 2.8023 - acc: 0.9094 - 273ms/step\n",
      "step  50/273 - loss: 16.2697 - acc: 0.9062 - 274ms/step\n",
      "step  60/273 - loss: 5.1045e-05 - acc: 0.9104 - 274ms/step\n",
      "step  70/273 - loss: 4.0600 - acc: 0.9107 - 274ms/step\n",
      "step  80/273 - loss: 5.7159 - acc: 0.9102 - 274ms/step\n",
      "step  90/273 - loss: 4.9089 - acc: 0.9125 - 273ms/step\n",
      "step 100/273 - loss: 3.0975 - acc: 0.9100 - 273ms/step\n",
      "step 110/273 - loss: 1.0547 - acc: 0.9125 - 274ms/step\n",
      "step 120/273 - loss: 15.5793 - acc: 0.9083 - 273ms/step\n",
      "step 130/273 - loss: 3.5744 - acc: 0.9072 - 273ms/step\n",
      "step 140/273 - loss: 2.1301 - acc: 0.9049 - 273ms/step\n",
      "step 150/273 - loss: 1.1473 - acc: 0.9012 - 273ms/step\n",
      "step 160/273 - loss: 4.7950 - acc: 0.8953 - 273ms/step\n",
      "step 170/273 - loss: 2.9802e-08 - acc: 0.8963 - 273ms/step\n",
      "step 180/273 - loss: 0.0186 - acc: 0.8986 - 273ms/step\n",
      "step 190/273 - loss: 3.3440 - acc: 0.8990 - 273ms/step\n",
      "step 200/273 - loss: 6.6212 - acc: 0.8997 - 273ms/step\n",
      "step 210/273 - loss: 2.0395 - acc: 0.8973 - 273ms/step\n",
      "step 220/273 - loss: 6.5667 - acc: 0.8963 - 273ms/step\n",
      "step 230/273 - loss: 2.0416 - acc: 0.8948 - 273ms/step\n",
      "step 240/273 - loss: 0.1394 - acc: 0.8948 - 273ms/step\n",
      "step 250/273 - loss: 6.3430 - acc: 0.8922 - 273ms/step\n",
      "step 260/273 - loss: 4.6200 - acc: 0.8918 - 273ms/step\n",
      "step 270/273 - loss: 5.5025 - acc: 0.8921 - 273ms/step\n",
      "step 273/273 - loss: 4.3921 - acc: 0.8917 - 273ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 0.8124 - acc: 0.8688 - 282ms/step\n",
      "step 20/39 - loss: 10.6623 - acc: 0.8875 - 280ms/step\n",
      "step 30/39 - loss: 5.3915 - acc: 0.9000 - 279ms/step\n",
      "step 39/39 - loss: 7.5563 - acc: 0.8894 - 278ms/step\n",
      "Eval samples: 624\n",
      "Epoch 3/15\n",
      "step  10/273 - loss: 4.1871e-06 - acc: 0.8875 - 278ms/step\n",
      "step  20/273 - loss: 0.0000e+00 - acc: 0.9156 - 275ms/step\n",
      "step  30/273 - loss: 5.0180 - acc: 0.9146 - 275ms/step\n",
      "step  40/273 - loss: 3.8633 - acc: 0.9172 - 274ms/step\n",
      "step  50/273 - loss: 19.4248 - acc: 0.9237 - 274ms/step\n",
      "step  60/273 - loss: 0.0000e+00 - acc: 0.9187 - 274ms/step\n",
      "step  70/273 - loss: 9.2612e-04 - acc: 0.9143 - 274ms/step\n",
      "step  80/273 - loss: 3.3377 - acc: 0.9125 - 273ms/step\n",
      "step  90/273 - loss: 15.3288 - acc: 0.9090 - 273ms/step\n",
      "step 100/273 - loss: 1.5782 - acc: 0.9113 - 273ms/step\n",
      "step 110/273 - loss: 13.2343 - acc: 0.9119 - 273ms/step\n",
      "step 120/273 - loss: 1.9899 - acc: 0.9141 - 274ms/step\n",
      "step 130/273 - loss: 4.6410 - acc: 0.9135 - 274ms/step\n",
      "step 140/273 - loss: 0.0000e+00 - acc: 0.9125 - 273ms/step\n",
      "step 150/273 - loss: 4.5445 - acc: 0.9117 - 274ms/step\n",
      "step 160/273 - loss: 5.2615 - acc: 0.9105 - 274ms/step\n",
      "step 170/273 - loss: 12.1804 - acc: 0.9099 - 274ms/step\n",
      "step 180/273 - loss: 0.9826 - acc: 0.9104 - 274ms/step\n",
      "step 190/273 - loss: 1.2774 - acc: 0.9109 - 274ms/step\n",
      "step 200/273 - loss: 10.6850 - acc: 0.9109 - 274ms/step\n",
      "step 210/273 - loss: 0.5634 - acc: 0.9104 - 274ms/step\n",
      "step 220/273 - loss: 0.0459 - acc: 0.9102 - 274ms/step\n",
      "step 230/273 - loss: 4.4004 - acc: 0.9098 - 274ms/step\n",
      "step 240/273 - loss: 12.6684 - acc: 0.9107 - 274ms/step\n",
      "step 250/273 - loss: 0.0000e+00 - acc: 0.9103 - 274ms/step\n",
      "step 260/273 - loss: 5.9426 - acc: 0.9099 - 274ms/step\n",
      "step 270/273 - loss: 1.6837 - acc: 0.9100 - 274ms/step\n",
      "step 273/273 - loss: 8.3486 - acc: 0.9107 - 274ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 22.1435 - acc: 0.8688 - 278ms/step\n",
      "step 20/39 - loss: 3.0502 - acc: 0.8781 - 277ms/step\n",
      "step 30/39 - loss: 0.0000e+00 - acc: 0.8917 - 277ms/step\n",
      "step 39/39 - loss: 11.9506 - acc: 0.8974 - 277ms/step\n",
      "Eval samples: 624\n",
      "Epoch 4/15\n",
      "step  10/273 - loss: 3.3449 - acc: 0.9313 - 276ms/step\n",
      "step  20/273 - loss: 4.9321e-06 - acc: 0.9250 - 277ms/step\n",
      "step  30/273 - loss: 2.5291 - acc: 0.9333 - 276ms/step\n",
      "step  40/273 - loss: 0.6201 - acc: 0.9344 - 274ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9363 - 274ms/step\n",
      "step  60/273 - loss: 0.7815 - acc: 0.9344 - 274ms/step\n",
      "step  70/273 - loss: 3.9989 - acc: 0.9330 - 274ms/step\n",
      "step  80/273 - loss: 5.7541 - acc: 0.9328 - 274ms/step\n",
      "step  90/273 - loss: 3.5787 - acc: 0.9306 - 274ms/step\n",
      "step 100/273 - loss: 0.4839 - acc: 0.9275 - 274ms/step\n",
      "step 110/273 - loss: 14.9756 - acc: 0.9267 - 274ms/step\n",
      "step 120/273 - loss: 9.6387 - acc: 0.9224 - 274ms/step\n",
      "step 130/273 - loss: 0.0135 - acc: 0.9236 - 274ms/step\n",
      "step 140/273 - loss: 4.2043 - acc: 0.9232 - 273ms/step\n",
      "step 150/273 - loss: 3.1640 - acc: 0.9242 - 273ms/step\n",
      "step 160/273 - loss: 0.0000e+00 - acc: 0.9246 - 274ms/step\n",
      "step 170/273 - loss: 6.2327 - acc: 0.9261 - 274ms/step\n",
      "step 180/273 - loss: 0.0000e+00 - acc: 0.9267 - 274ms/step\n",
      "step 190/273 - loss: 10.8979 - acc: 0.9270 - 274ms/step\n",
      "step 200/273 - loss: 2.1360 - acc: 0.9241 - 274ms/step\n",
      "step 210/273 - loss: 6.1465 - acc: 0.9232 - 274ms/step\n",
      "step 220/273 - loss: 0.0075 - acc: 0.9230 - 274ms/step\n",
      "step 230/273 - loss: 2.8775 - acc: 0.9223 - 274ms/step\n",
      "step 240/273 - loss: 0.0000e+00 - acc: 0.9227 - 274ms/step\n",
      "step 250/273 - loss: 1.3518 - acc: 0.9223 - 274ms/step\n",
      "step 260/273 - loss: 3.4755 - acc: 0.9228 - 274ms/step\n",
      "step 270/273 - loss: 7.1077 - acc: 0.9225 - 274ms/step\n",
      "step 273/273 - loss: 0.0000e+00 - acc: 0.9224 - 274ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 11.6239 - acc: 0.8375 - 278ms/step\n",
      "step 20/39 - loss: 7.2954 - acc: 0.8844 - 278ms/step\n",
      "step 30/39 - loss: 1.0659 - acc: 0.8812 - 278ms/step\n",
      "step 39/39 - loss: 4.7049 - acc: 0.8798 - 278ms/step\n",
      "Eval samples: 624\n",
      "Epoch 5/15\n",
      "step  10/273 - loss: 60.0977 - acc: 0.9125 - 279ms/step\n",
      "step  20/273 - loss: 0.9402 - acc: 0.9187 - 277ms/step\n",
      "step  30/273 - loss: 0.0000e+00 - acc: 0.9271 - 277ms/step\n",
      "step  40/273 - loss: 2.6088 - acc: 0.9313 - 276ms/step\n",
      "step  50/273 - loss: 1.2034 - acc: 0.9375 - 276ms/step\n",
      "step  60/273 - loss: 5.1409e-07 - acc: 0.9406 - 276ms/step\n",
      "step  70/273 - loss: 0.8498 - acc: 0.9446 - 276ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9453 - 276ms/step\n",
      "step  90/273 - loss: 0.0000e+00 - acc: 0.9444 - 276ms/step\n",
      "step 100/273 - loss: 4.2266 - acc: 0.9463 - 276ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9455 - 276ms/step\n",
      "step 120/273 - loss: 1.1503e-05 - acc: 0.9453 - 276ms/step\n",
      "step 130/273 - loss: 0.0248 - acc: 0.9462 - 276ms/step\n",
      "step 140/273 - loss: 11.9729 - acc: 0.9442 - 276ms/step\n",
      "step 150/273 - loss: 2.0715 - acc: 0.9417 - 276ms/step\n",
      "step 160/273 - loss: 2.8747 - acc: 0.9406 - 276ms/step\n",
      "step 170/273 - loss: 5.6145 - acc: 0.9415 - 276ms/step\n",
      "step 180/273 - loss: 0.0816 - acc: 0.9413 - 276ms/step\n",
      "step 190/273 - loss: 2.7754 - acc: 0.9408 - 276ms/step\n",
      "step 200/273 - loss: 7.5461 - acc: 0.9400 - 275ms/step\n",
      "step 210/273 - loss: 2.8936 - acc: 0.9387 - 275ms/step\n",
      "step 220/273 - loss: 0.0000e+00 - acc: 0.9403 - 275ms/step\n",
      "step 230/273 - loss: 0.3636 - acc: 0.9391 - 275ms/step\n",
      "step 240/273 - loss: 9.9495 - acc: 0.9372 - 275ms/step\n",
      "step 250/273 - loss: 0.0000e+00 - acc: 0.9375 - 275ms/step\n",
      "step 260/273 - loss: 0.0000e+00 - acc: 0.9380 - 275ms/step\n",
      "step 270/273 - loss: 1.1699 - acc: 0.9370 - 275ms/step\n",
      "step 273/273 - loss: 2.6102 - acc: 0.9373 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 0.0000e+00 - acc: 0.8938 - 278ms/step\n",
      "step 20/39 - loss: 0.0000e+00 - acc: 0.8906 - 277ms/step\n",
      "step 30/39 - loss: 6.6521e-04 - acc: 0.8812 - 276ms/step\n",
      "step 39/39 - loss: 1.1633 - acc: 0.8878 - 276ms/step\n",
      "Eval samples: 624\n",
      "Epoch 6/15\n",
      "step  10/273 - loss: 0.0000e+00 - acc: 0.9313 - 276ms/step\n",
      "step  20/273 - loss: 0.0000e+00 - acc: 0.9531 - 275ms/step\n",
      "step  30/273 - loss: 5.7841 - acc: 0.9521 - 274ms/step\n",
      "step  40/273 - loss: 0.0000e+00 - acc: 0.9469 - 274ms/step\n",
      "step  50/273 - loss: 7.1596e-06 - acc: 0.9450 - 274ms/step\n",
      "step  60/273 - loss: 2.4587e-07 - acc: 0.9448 - 274ms/step\n",
      "step  70/273 - loss: 0.0013 - acc: 0.9437 - 274ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9430 - 274ms/step\n",
      "step  90/273 - loss: 2.0189 - acc: 0.9431 - 274ms/step\n",
      "step 100/273 - loss: 0.0062 - acc: 0.9437 - 274ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9449 - 274ms/step\n",
      "step 120/273 - loss: 7.9724 - acc: 0.9464 - 274ms/step\n",
      "step 130/273 - loss: 7.6510 - acc: 0.9442 - 274ms/step\n",
      "step 140/273 - loss: 0.0000e+00 - acc: 0.9437 - 274ms/step\n",
      "step 150/273 - loss: 3.9704 - acc: 0.9417 - 274ms/step\n",
      "step 160/273 - loss: 7.8035 - acc: 0.9422 - 275ms/step\n",
      "step 170/273 - loss: 6.4640 - acc: 0.9404 - 275ms/step\n",
      "step 180/273 - loss: 1.3461 - acc: 0.9403 - 275ms/step\n",
      "step 190/273 - loss: 0.2008 - acc: 0.9408 - 275ms/step\n",
      "step 200/273 - loss: 9.5959 - acc: 0.9406 - 275ms/step\n",
      "step 210/273 - loss: 0.0000e+00 - acc: 0.9414 - 275ms/step\n",
      "step 220/273 - loss: 0.0000e+00 - acc: 0.9426 - 275ms/step\n",
      "step 230/273 - loss: 9.5886 - acc: 0.9429 - 275ms/step\n",
      "step 240/273 - loss: 21.7805 - acc: 0.9443 - 275ms/step\n",
      "step 250/273 - loss: 0.9707 - acc: 0.9440 - 275ms/step\n",
      "step 260/273 - loss: 1.2276 - acc: 0.9430 - 275ms/step\n",
      "step 270/273 - loss: 2.5541 - acc: 0.9410 - 275ms/step\n",
      "step 273/273 - loss: 3.0167e-04 - acc: 0.9412 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 0.4287 - acc: 0.9062 - 280ms/step\n",
      "step 20/39 - loss: 0.0000e+00 - acc: 0.9062 - 278ms/step\n",
      "step 30/39 - loss: 9.5728 - acc: 0.9042 - 277ms/step\n",
      "step 39/39 - loss: 4.6408 - acc: 0.9103 - 276ms/step\n",
      "Eval samples: 624\n",
      "Epoch 7/15\n",
      "step  10/273 - loss: 6.5780 - acc: 0.9563 - 280ms/step\n",
      "step  20/273 - loss: 8.5315 - acc: 0.9344 - 277ms/step\n",
      "step  30/273 - loss: 0.0000e+00 - acc: 0.9333 - 277ms/step\n",
      "step  40/273 - loss: 3.5237 - acc: 0.9359 - 277ms/step\n",
      "step  50/273 - loss: 1.4901e-07 - acc: 0.9413 - 275ms/step\n",
      "step  60/273 - loss: 2.3816 - acc: 0.9458 - 276ms/step\n",
      "step  70/273 - loss: 4.1692 - acc: 0.9473 - 276ms/step\n",
      "step  80/273 - loss: 2.5513 - acc: 0.9477 - 276ms/step\n",
      "step  90/273 - loss: 1.2779 - acc: 0.9458 - 276ms/step\n",
      "step 100/273 - loss: 1.7669 - acc: 0.9431 - 276ms/step\n",
      "step 110/273 - loss: 4.8138 - acc: 0.9403 - 276ms/step\n",
      "step 120/273 - loss: 15.1933 - acc: 0.9401 - 275ms/step\n",
      "step 130/273 - loss: 0.8160 - acc: 0.9404 - 275ms/step\n",
      "step 140/273 - loss: 0.7380 - acc: 0.9379 - 275ms/step\n",
      "step 150/273 - loss: 6.2626 - acc: 0.9392 - 275ms/step\n",
      "step 160/273 - loss: 0.5667 - acc: 0.9383 - 275ms/step\n",
      "step 170/273 - loss: 0.0000e+00 - acc: 0.9404 - 275ms/step\n",
      "step 180/273 - loss: 0.0000e+00 - acc: 0.9427 - 275ms/step\n",
      "step 190/273 - loss: 1.4901e-07 - acc: 0.9434 - 275ms/step\n",
      "step 200/273 - loss: 11.6266 - acc: 0.9428 - 275ms/step\n",
      "step 210/273 - loss: 0.0000e+00 - acc: 0.9426 - 275ms/step\n",
      "step 220/273 - loss: 1.6612 - acc: 0.9409 - 275ms/step\n",
      "step 230/273 - loss: 9.0469 - acc: 0.9413 - 275ms/step\n",
      "step 240/273 - loss: 0.0000e+00 - acc: 0.9424 - 275ms/step\n",
      "step 250/273 - loss: 2.7865e-06 - acc: 0.9423 - 275ms/step\n",
      "step 260/273 - loss: 5.5654 - acc: 0.9423 - 275ms/step\n",
      "step 270/273 - loss: 0.0000e+00 - acc: 0.9431 - 275ms/step\n",
      "step 273/273 - loss: 5.1843 - acc: 0.9428 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 0.0000e+00 - acc: 0.8875 - 282ms/step\n",
      "step 20/39 - loss: 10.5573 - acc: 0.9031 - 281ms/step\n",
      "step 30/39 - loss: 11.1922 - acc: 0.9083 - 280ms/step\n",
      "step 39/39 - loss: 16.9323 - acc: 0.9087 - 279ms/step\n",
      "Eval samples: 624\n",
      "Epoch 8/15\n",
      "step  10/273 - loss: 0.0000e+00 - acc: 0.9625 - 281ms/step\n",
      "step  20/273 - loss: 0.0000e+00 - acc: 0.9656 - 279ms/step\n",
      "step  30/273 - loss: 6.1170 - acc: 0.9604 - 278ms/step\n",
      "step  40/273 - loss: 7.8295 - acc: 0.9563 - 277ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9537 - 277ms/step\n",
      "step  60/273 - loss: 15.6801 - acc: 0.9490 - 277ms/step\n",
      "step  70/273 - loss: 0.0000e+00 - acc: 0.9500 - 277ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9484 - 276ms/step\n",
      "step  90/273 - loss: 1.5830e-05 - acc: 0.9493 - 276ms/step\n",
      "step 100/273 - loss: 1.9254 - acc: 0.9487 - 276ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9477 - 275ms/step\n",
      "step 120/273 - loss: 0.0000e+00 - acc: 0.9484 - 275ms/step\n",
      "step 130/273 - loss: 5.2154e-08 - acc: 0.9490 - 275ms/step\n",
      "step 140/273 - loss: 2.8742 - acc: 0.9469 - 275ms/step\n",
      "step 150/273 - loss: 5.0994 - acc: 0.9475 - 275ms/step\n",
      "step 160/273 - loss: 0.0000e+00 - acc: 0.9496 - 275ms/step\n",
      "step 170/273 - loss: 9.4622e-07 - acc: 0.9504 - 275ms/step\n",
      "step 180/273 - loss: 21.1679 - acc: 0.9503 - 275ms/step\n",
      "step 190/273 - loss: 16.0331 - acc: 0.9503 - 275ms/step\n",
      "step 200/273 - loss: 0.0000e+00 - acc: 0.9506 - 275ms/step\n",
      "step 210/273 - loss: 0.0000e+00 - acc: 0.9509 - 275ms/step\n",
      "step 220/273 - loss: 5.5095 - acc: 0.9503 - 275ms/step\n",
      "step 230/273 - loss: 14.1003 - acc: 0.9505 - 275ms/step\n",
      "step 240/273 - loss: 3.4090 - acc: 0.9495 - 275ms/step\n",
      "step 250/273 - loss: 5.0552 - acc: 0.9493 - 275ms/step\n",
      "step 260/273 - loss: 0.9803 - acc: 0.9493 - 275ms/step\n",
      "step 270/273 - loss: 12.4450 - acc: 0.9486 - 275ms/step\n",
      "step 273/273 - loss: 0.3682 - acc: 0.9487 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 6.6528 - acc: 0.9250 - 280ms/step\n",
      "step 20/39 - loss: 1.6856 - acc: 0.9000 - 277ms/step\n",
      "step 30/39 - loss: 0.0000e+00 - acc: 0.9125 - 278ms/step\n",
      "step 39/39 - loss: 2.8573 - acc: 0.9119 - 278ms/step\n",
      "Eval samples: 624\n",
      "Epoch 9/15\n",
      "step  10/273 - loss: 0.0000e+00 - acc: 0.9750 - 277ms/step\n",
      "step  20/273 - loss: 2.3786e-05 - acc: 0.9656 - 276ms/step\n",
      "step  30/273 - loss: 5.0611 - acc: 0.9563 - 276ms/step\n",
      "step  40/273 - loss: 0.0000e+00 - acc: 0.9531 - 276ms/step\n",
      "step  50/273 - loss: 4.2512 - acc: 0.9500 - 276ms/step\n",
      "step  60/273 - loss: 0.0000e+00 - acc: 0.9531 - 276ms/step\n",
      "step  70/273 - loss: 0.0000e+00 - acc: 0.9518 - 277ms/step\n",
      "step  80/273 - loss: 0.3779 - acc: 0.9508 - 276ms/step\n",
      "step  90/273 - loss: 1.2200e-04 - acc: 0.9507 - 276ms/step\n",
      "step 100/273 - loss: 0.0000e+00 - acc: 0.9513 - 276ms/step\n",
      "step 110/273 - loss: 1.4189 - acc: 0.9540 - 276ms/step\n",
      "step 120/273 - loss: 5.3062 - acc: 0.9536 - 276ms/step\n",
      "step 130/273 - loss: 6.6798 - acc: 0.9529 - 276ms/step\n",
      "step 140/273 - loss: 3.7038 - acc: 0.9540 - 276ms/step\n",
      "step 150/273 - loss: 4.7730 - acc: 0.9554 - 275ms/step\n",
      "step 160/273 - loss: 3.8824 - acc: 0.9543 - 275ms/step\n",
      "step 170/273 - loss: 0.9191 - acc: 0.9548 - 275ms/step\n",
      "step 180/273 - loss: 0.0000e+00 - acc: 0.9556 - 275ms/step\n",
      "step 190/273 - loss: 10.1610 - acc: 0.9549 - 275ms/step\n",
      "step 200/273 - loss: 0.4389 - acc: 0.9559 - 275ms/step\n",
      "step 210/273 - loss: 0.0000e+00 - acc: 0.9557 - 275ms/step\n",
      "step 220/273 - loss: 0.3843 - acc: 0.9548 - 275ms/step\n",
      "step 230/273 - loss: 1.4198 - acc: 0.9527 - 275ms/step\n",
      "step 240/273 - loss: 3.1214 - acc: 0.9513 - 275ms/step\n",
      "step 250/273 - loss: 2.5732 - acc: 0.9505 - 275ms/step\n",
      "step 260/273 - loss: 0.0000e+00 - acc: 0.9507 - 275ms/step\n",
      "step 270/273 - loss: 0.9279 - acc: 0.9516 - 275ms/step\n",
      "step 273/273 - loss: 3.5120 - acc: 0.9517 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 0.0000e+00 - acc: 0.9250 - 280ms/step\n",
      "step 20/39 - loss: 0.0000e+00 - acc: 0.9125 - 279ms/step\n",
      "step 30/39 - loss: 14.7478 - acc: 0.9104 - 279ms/step\n",
      "step 39/39 - loss: 10.9964 - acc: 0.9022 - 278ms/step\n",
      "Eval samples: 624\n",
      "Epoch 10/15\n",
      "step  10/273 - loss: 10.4963 - acc: 0.9563 - 277ms/step\n",
      "step  20/273 - loss: 1.6780 - acc: 0.9656 - 277ms/step\n",
      "step  30/273 - loss: 1.9371e-07 - acc: 0.9667 - 276ms/step\n",
      "step  40/273 - loss: 1.3472 - acc: 0.9641 - 276ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9637 - 276ms/step\n",
      "step  60/273 - loss: 8.2750 - acc: 0.9635 - 276ms/step\n",
      "step  70/273 - loss: 5.3567 - acc: 0.9616 - 276ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9602 - 275ms/step\n",
      "step  90/273 - loss: 0.0000e+00 - acc: 0.9611 - 275ms/step\n",
      "step 100/273 - loss: 12.9596 - acc: 0.9619 - 275ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9614 - 275ms/step\n",
      "step 120/273 - loss: 0.0725 - acc: 0.9615 - 275ms/step\n",
      "step 130/273 - loss: 0.0000e+00 - acc: 0.9620 - 275ms/step\n",
      "step 140/273 - loss: 3.6368 - acc: 0.9594 - 275ms/step\n",
      "step 150/273 - loss: 0.0000e+00 - acc: 0.9613 - 275ms/step\n",
      "step 160/273 - loss: 0.0000e+00 - acc: 0.9605 - 275ms/step\n",
      "step 170/273 - loss: 0.0000e+00 - acc: 0.9610 - 275ms/step\n",
      "step 180/273 - loss: 15.8561 - acc: 0.9615 - 275ms/step\n",
      "step 190/273 - loss: 1.6953 - acc: 0.9612 - 275ms/step\n",
      "step 200/273 - loss: 0.0000e+00 - acc: 0.9603 - 275ms/step\n",
      "step 210/273 - loss: 7.0423 - acc: 0.9592 - 275ms/step\n",
      "step 220/273 - loss: 0.0000e+00 - acc: 0.9594 - 275ms/step\n",
      "step 230/273 - loss: 1.5803 - acc: 0.9582 - 275ms/step\n",
      "step 240/273 - loss: 0.0000e+00 - acc: 0.9583 - 275ms/step\n",
      "step 250/273 - loss: 0.0000e+00 - acc: 0.9587 - 275ms/step\n",
      "step 260/273 - loss: 2.8856e-04 - acc: 0.9582 - 275ms/step\n",
      "step 270/273 - loss: 13.5749 - acc: 0.9586 - 275ms/step\n",
      "step 273/273 - loss: 0.0000e+00 - acc: 0.9588 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 18.6853 - acc: 0.9187 - 276ms/step\n",
      "step 20/39 - loss: 4.9515 - acc: 0.9094 - 276ms/step\n",
      "step 30/39 - loss: 1.9499 - acc: 0.8979 - 276ms/step\n",
      "step 39/39 - loss: 3.4162 - acc: 0.9054 - 275ms/step\n",
      "Eval samples: 624\n",
      "Epoch 11/15\n",
      "step  10/273 - loss: 8.9129 - acc: 0.9750 - 279ms/step\n",
      "step  20/273 - loss: 0.0000e+00 - acc: 0.9719 - 277ms/step\n",
      "step  30/273 - loss: 10.1919 - acc: 0.9625 - 276ms/step\n",
      "step  40/273 - loss: 0.0000e+00 - acc: 0.9672 - 275ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9650 - 275ms/step\n",
      "step  60/273 - loss: 0.0000e+00 - acc: 0.9667 - 275ms/step\n",
      "step  70/273 - loss: 10.9312 - acc: 0.9696 - 275ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9672 - 275ms/step\n",
      "step  90/273 - loss: 1.0327 - acc: 0.9674 - 275ms/step\n",
      "step 100/273 - loss: 0.0000e+00 - acc: 0.9650 - 275ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9648 - 275ms/step\n",
      "step 120/273 - loss: 0.0000e+00 - acc: 0.9651 - 275ms/step\n",
      "step 130/273 - loss: 14.7466 - acc: 0.9659 - 274ms/step\n",
      "step 140/273 - loss: 1.3951 - acc: 0.9679 - 274ms/step\n",
      "step 150/273 - loss: 0.5884 - acc: 0.9679 - 274ms/step\n",
      "step 160/273 - loss: 0.0000e+00 - acc: 0.9680 - 274ms/step\n",
      "step 170/273 - loss: 0.0000e+00 - acc: 0.9688 - 274ms/step\n",
      "step 180/273 - loss: 0.0000e+00 - acc: 0.9677 - 274ms/step\n",
      "step 190/273 - loss: 7.6278e-05 - acc: 0.9671 - 274ms/step\n",
      "step 200/273 - loss: 3.6704 - acc: 0.9672 - 275ms/step\n",
      "step 210/273 - loss: 0.5221 - acc: 0.9676 - 275ms/step\n",
      "step 220/273 - loss: 11.6493 - acc: 0.9673 - 275ms/step\n",
      "step 230/273 - loss: 5.4595 - acc: 0.9663 - 275ms/step\n",
      "step 240/273 - loss: 0.6232 - acc: 0.9661 - 275ms/step\n",
      "step 250/273 - loss: 0.0000e+00 - acc: 0.9665 - 275ms/step\n",
      "step 260/273 - loss: 0.0000e+00 - acc: 0.9671 - 275ms/step\n",
      "step 270/273 - loss: 8.8332 - acc: 0.9667 - 275ms/step\n",
      "step 273/273 - loss: 0.0000e+00 - acc: 0.9666 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 8.8146 - acc: 0.9000 - 284ms/step\n",
      "step 20/39 - loss: 27.7745 - acc: 0.9062 - 281ms/step\n",
      "step 30/39 - loss: 16.9375 - acc: 0.8938 - 280ms/step\n",
      "step 39/39 - loss: 8.6818 - acc: 0.9054 - 279ms/step\n",
      "Eval samples: 624\n",
      "Epoch 12/15\n",
      "step  10/273 - loss: 3.5656 - acc: 0.9563 - 278ms/step\n",
      "step  20/273 - loss: 0.0000e+00 - acc: 0.9563 - 274ms/step\n",
      "step  30/273 - loss: 2.8414 - acc: 0.9458 - 275ms/step\n",
      "step  40/273 - loss: 1.4901e-08 - acc: 0.9531 - 275ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9550 - 275ms/step\n",
      "step  60/273 - loss: 0.0000e+00 - acc: 0.9583 - 275ms/step\n",
      "step  70/273 - loss: 2.9142e-04 - acc: 0.9607 - 275ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9609 - 275ms/step\n",
      "step  90/273 - loss: 10.1228 - acc: 0.9604 - 275ms/step\n",
      "step 100/273 - loss: 12.4487 - acc: 0.9613 - 275ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9597 - 275ms/step\n",
      "step 120/273 - loss: 0.0000e+00 - acc: 0.9604 - 275ms/step\n",
      "step 130/273 - loss: 0.0000e+00 - acc: 0.9615 - 275ms/step\n",
      "step 140/273 - loss: 0.0000e+00 - acc: 0.9612 - 275ms/step\n",
      "step 150/273 - loss: 4.8207 - acc: 0.9600 - 275ms/step\n",
      "step 160/273 - loss: 10.1517 - acc: 0.9602 - 275ms/step\n",
      "step 170/273 - loss: 27.9346 - acc: 0.9603 - 275ms/step\n",
      "step 180/273 - loss: 0.0000e+00 - acc: 0.9608 - 275ms/step\n",
      "step 190/273 - loss: 0.0374 - acc: 0.9615 - 275ms/step\n",
      "step 200/273 - loss: 0.0000e+00 - acc: 0.9616 - 275ms/step\n",
      "step 210/273 - loss: 9.3196 - acc: 0.9616 - 275ms/step\n",
      "step 220/273 - loss: 0.0000e+00 - acc: 0.9614 - 275ms/step\n",
      "step 230/273 - loss: 0.8782 - acc: 0.9603 - 275ms/step\n",
      "step 240/273 - loss: 0.0000e+00 - acc: 0.9599 - 275ms/step\n",
      "step 250/273 - loss: 4.2976 - acc: 0.9575 - 275ms/step\n",
      "step 260/273 - loss: 25.8097 - acc: 0.9565 - 275ms/step\n",
      "step 270/273 - loss: 28.4328 - acc: 0.9560 - 275ms/step\n",
      "step 273/273 - loss: 20.5853 - acc: 0.9556 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 23.6287 - acc: 0.9187 - 281ms/step\n",
      "step 20/39 - loss: 5.2596 - acc: 0.9094 - 280ms/step\n",
      "step 30/39 - loss: 28.3345 - acc: 0.9021 - 279ms/step\n",
      "step 39/39 - loss: 7.9374 - acc: 0.8958 - 278ms/step\n",
      "Eval samples: 624\n",
      "Epoch 13/15\n",
      "step  10/273 - loss: 7.4506e-09 - acc: 0.9500 - 285ms/step\n",
      "step  20/273 - loss: 0.0000e+00 - acc: 0.9531 - 280ms/step\n",
      "step  30/273 - loss: 0.0000e+00 - acc: 0.9521 - 280ms/step\n",
      "step  40/273 - loss: 0.0000e+00 - acc: 0.9594 - 278ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9537 - 279ms/step\n",
      "step  60/273 - loss: 7.5133 - acc: 0.9563 - 278ms/step\n",
      "step  70/273 - loss: 0.0000e+00 - acc: 0.9589 - 278ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9617 - 277ms/step\n",
      "step  90/273 - loss: 10.8226 - acc: 0.9590 - 277ms/step\n",
      "step 100/273 - loss: 0.0000e+00 - acc: 0.9606 - 277ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9619 - 276ms/step\n",
      "step 120/273 - loss: 7.4506e-09 - acc: 0.9625 - 276ms/step\n",
      "step 130/273 - loss: 4.4357 - acc: 0.9625 - 276ms/step\n",
      "step 140/273 - loss: 2.6020 - acc: 0.9616 - 276ms/step\n",
      "step 150/273 - loss: 2.0073 - acc: 0.9600 - 276ms/step\n",
      "step 160/273 - loss: 8.6774 - acc: 0.9609 - 275ms/step\n",
      "step 170/273 - loss: 0.0000e+00 - acc: 0.9614 - 275ms/step\n",
      "step 180/273 - loss: 22.5032 - acc: 0.9615 - 275ms/step\n",
      "step 190/273 - loss: 0.4055 - acc: 0.9618 - 275ms/step\n",
      "step 200/273 - loss: 0.8862 - acc: 0.9622 - 275ms/step\n",
      "step 210/273 - loss: 6.7055e-08 - acc: 0.9628 - 275ms/step\n",
      "step 220/273 - loss: 1.8485 - acc: 0.9628 - 275ms/step\n",
      "step 230/273 - loss: 19.7752 - acc: 0.9617 - 275ms/step\n",
      "step 240/273 - loss: 15.8698 - acc: 0.9620 - 276ms/step\n",
      "step 250/273 - loss: 0.0361 - acc: 0.9617 - 276ms/step\n",
      "step 260/273 - loss: 0.9417 - acc: 0.9611 - 276ms/step\n",
      "step 270/273 - loss: 0.0000e+00 - acc: 0.9618 - 275ms/step\n",
      "step 273/273 - loss: 0.0000e+00 - acc: 0.9622 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 0.0000e+00 - acc: 0.8562 - 276ms/step\n",
      "step 20/39 - loss: 22.7506 - acc: 0.8688 - 276ms/step\n",
      "step 30/39 - loss: 20.2257 - acc: 0.8708 - 277ms/step\n",
      "step 39/39 - loss: 26.6941 - acc: 0.8766 - 277ms/step\n",
      "Eval samples: 624\n",
      "Epoch 14/15\n",
      "step  10/273 - loss: 0.0000e+00 - acc: 0.9625 - 281ms/step\n",
      "step  20/273 - loss: 19.4213 - acc: 0.9656 - 279ms/step\n",
      "step  30/273 - loss: 12.9565 - acc: 0.9646 - 278ms/step\n",
      "step  40/273 - loss: 0.0768 - acc: 0.9641 - 278ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9637 - 278ms/step\n",
      "step  60/273 - loss: 0.0000e+00 - acc: 0.9656 - 278ms/step\n",
      "step  70/273 - loss: 7.6080 - acc: 0.9643 - 277ms/step\n",
      "step  80/273 - loss: 0.0000e+00 - acc: 0.9648 - 277ms/step\n",
      "step  90/273 - loss: 0.0000e+00 - acc: 0.9660 - 277ms/step\n",
      "step 100/273 - loss: 0.0000e+00 - acc: 0.9663 - 277ms/step\n",
      "step 110/273 - loss: 31.5347 - acc: 0.9648 - 277ms/step\n",
      "step 120/273 - loss: 2.3330 - acc: 0.9661 - 276ms/step\n",
      "step 130/273 - loss: 3.8154 - acc: 0.9659 - 276ms/step\n",
      "step 140/273 - loss: 4.5960 - acc: 0.9638 - 276ms/step\n",
      "step 150/273 - loss: 0.0013 - acc: 0.9650 - 276ms/step\n",
      "step 160/273 - loss: 7.8298 - acc: 0.9656 - 276ms/step\n",
      "step 170/273 - loss: 67.0202 - acc: 0.9640 - 276ms/step\n",
      "step 180/273 - loss: 0.0000e+00 - acc: 0.9653 - 276ms/step\n",
      "step 190/273 - loss: 0.0000e+00 - acc: 0.9651 - 276ms/step\n",
      "step 200/273 - loss: 4.2337 - acc: 0.9647 - 276ms/step\n",
      "step 210/273 - loss: 9.7213 - acc: 0.9640 - 276ms/step\n",
      "step 220/273 - loss: 4.6603 - acc: 0.9634 - 276ms/step\n",
      "step 230/273 - loss: 1.9788 - acc: 0.9639 - 276ms/step\n",
      "step 240/273 - loss: 1.2166 - acc: 0.9630 - 276ms/step\n",
      "step 250/273 - loss: 8.0163 - acc: 0.9625 - 275ms/step\n",
      "step 260/273 - loss: 11.5379 - acc: 0.9615 - 275ms/step\n",
      "step 270/273 - loss: 0.0000e+00 - acc: 0.9600 - 275ms/step\n",
      "step 273/273 - loss: 0.2877 - acc: 0.9597 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 44.6821 - acc: 0.8938 - 281ms/step\n",
      "step 20/39 - loss: 26.5073 - acc: 0.8844 - 280ms/step\n",
      "step 30/39 - loss: 25.5151 - acc: 0.8875 - 279ms/step\n",
      "step 39/39 - loss: 26.0736 - acc: 0.8894 - 280ms/step\n",
      "Eval samples: 624\n",
      "Epoch 15/15\n",
      "step  10/273 - loss: 5.4647 - acc: 0.9250 - 280ms/step\n",
      "step  20/273 - loss: 0.0000e+00 - acc: 0.9469 - 277ms/step\n",
      "step  30/273 - loss: 0.0000e+00 - acc: 0.9500 - 276ms/step\n",
      "step  40/273 - loss: 3.5025 - acc: 0.9516 - 275ms/step\n",
      "step  50/273 - loss: 0.0000e+00 - acc: 0.9587 - 276ms/step\n",
      "step  60/273 - loss: 0.0000e+00 - acc: 0.9604 - 275ms/step\n",
      "step  70/273 - loss: 0.0000e+00 - acc: 0.9652 - 276ms/step\n",
      "step  80/273 - loss: 1.3008 - acc: 0.9641 - 276ms/step\n",
      "step  90/273 - loss: 0.0000e+00 - acc: 0.9660 - 276ms/step\n",
      "step 100/273 - loss: 0.0000e+00 - acc: 0.9656 - 275ms/step\n",
      "step 110/273 - loss: 0.0000e+00 - acc: 0.9659 - 276ms/step\n",
      "step 120/273 - loss: 20.9613 - acc: 0.9661 - 275ms/step\n",
      "step 130/273 - loss: 0.9466 - acc: 0.9649 - 275ms/step\n",
      "step 140/273 - loss: 5.4667 - acc: 0.9661 - 275ms/step\n",
      "step 150/273 - loss: 3.2040 - acc: 0.9658 - 275ms/step\n",
      "step 160/273 - loss: 2.0739 - acc: 0.9656 - 275ms/step\n",
      "step 170/273 - loss: 0.0000e+00 - acc: 0.9647 - 275ms/step\n",
      "step 180/273 - loss: 0.0000e+00 - acc: 0.9663 - 275ms/step\n",
      "step 190/273 - loss: 0.0000e+00 - acc: 0.9664 - 275ms/step\n",
      "step 200/273 - loss: 0.0000e+00 - acc: 0.9656 - 275ms/step\n",
      "step 210/273 - loss: 18.8621 - acc: 0.9646 - 275ms/step\n",
      "step 220/273 - loss: 1.2476 - acc: 0.9645 - 275ms/step\n",
      "step 230/273 - loss: 0.0000e+00 - acc: 0.9647 - 275ms/step\n",
      "step 240/273 - loss: 0.0000e+00 - acc: 0.9641 - 275ms/step\n",
      "step 250/273 - loss: 0.0000e+00 - acc: 0.9643 - 275ms/step\n",
      "step 260/273 - loss: 0.0000e+00 - acc: 0.9642 - 275ms/step\n",
      "step 270/273 - loss: 6.7381 - acc: 0.9634 - 275ms/step\n",
      "step 273/273 - loss: 8.3238 - acc: 0.9629 - 275ms/step\n",
      "Eval begin...\n",
      "step 10/39 - loss: 0.0000e+00 - acc: 0.9187 - 281ms/step\n",
      "step 20/39 - loss: 21.5116 - acc: 0.9094 - 281ms/step\n",
      "step 30/39 - loss: 11.3229 - acc: 0.9083 - 279ms/step\n",
      "step 39/39 - loss: 23.2216 - acc: 0.9087 - 279ms/step\n",
      "Eval samples: 624\n",
      "Eval begin...\n",
      "step 10/39 - loss: 18.1225 - acc: 0.9062 - 279ms/step\n",
      "step 20/39 - loss: 0.0000e+00 - acc: 0.9062 - 278ms/step\n",
      "step 30/39 - loss: 24.9194 - acc: 0.9083 - 278ms/step\n",
      "step 39/39 - loss: 0.0000e+00 - acc: 0.9087 - 278ms/step\n",
      "Eval samples: 624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.0], 'acc': 0.9086538461538461}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##优化器\n",
    "optim = paddle.optimizer.Adam(learning_rate=train_parameters['learning_strategy']['lr'], parameters=model.parameters())\n",
    "##为模型设置优化器，loss函数\n",
    "model.prepare(optim, paddle.nn.CrossEntropyLoss(),paddle.metric.Accuracy())\n",
    "\n",
    "log_callback = paddle.callbacks.VisualDL(log_dir='logdir_3') # 设置回调visualDL\n",
    "# model_callback = paddle.callbacks.ModelCheckpoint(save_dir='./model')\n",
    "\n",
    "model.fit(train_loader, test_loader, epochs=train_parameters['num_epochs'],callbacks=[log_callback],)\n",
    "\n",
    "model.save('model/model3')\n",
    "\n",
    "log_callback_test = paddle.callbacks.VisualDL(log_dir='logdir_test3') # 设置回调visualDL\n",
    "\n",
    "model.evaluate(test_loader,callbacks=[log_callback_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_450/1475625533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog_callback_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_loader,callbacks=[log_callback_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !visualdl service upload --logdir ./logdir_3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
